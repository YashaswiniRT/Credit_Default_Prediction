# Credit_Default_Prediction


●	Utilized Python to analyze a huge data set containing a set of 458,913 unique customers and 190 aggregated profile features used to determine the probability that a customer will default.
●	We utilized the feather module as the pickle module was slow, and over-serialized. At the same time, the parquet module was slow too despite a great way of handling complex data and features differently for more efficient data compression.
●	Performed Data cleaning, exploratory data analysis, and visualized data pre- and post-data processing.
●	Built and fit LightGBM, and XGBoost models both have faster training speed and efficiency and can handle large datasets. We also built logistic regression and random forest classifier models
●	Performed feature engineering by Imputers and utilized base learners like KNN, Naive Bayes, and Gradient Boosted Trees.
●	Contributed to strategic planning processes by providing data-driven insights and forecasting trends to achieve the project goal.
